# ğŸ©ºÂ Medical RAG AssistantÂ â€” LitQAâ€‘v2 Edition

> **From baseline to productionâ€‘ready:**Â +65â€¯% F1, citations on every answer,Â â‰¤â€¯11â€¯s median latency.

---

## 1Â Â Why this project matters

Biomedical questions need **upâ€‘toâ€‘date, fully cited** answers. The original *PaperHelper* demo proved the concept but shipped with:

* Denseâ€‘only retrieval â‡’ weak recall
* No reranking â‡’ noisy contexts
* Hallucinations in answers

This repo rebuilds the stack and adds measurable upgrades that raise LitQAâ€‘v2 scores from **0.23Â â†’Â 0.38Â F1** *without* expensive model fineâ€‘tunes.

---

## 2Â Â Key achievements

| Phase        | Upgrade                                | F1Â â–²     | LatencyÂ â–¼ | Notes             |
| ------------ | -------------------------------------- | -------- | --------- | ----------------- |
| **Baseline** | *PaperHelper* clone (denseâ€‘10)         | 0.23     | 4.8Â s     | public reference  |
| Retrieval    | **Hybrid** denseÂ 8Â +Â BM25Â 32 â€¢ RRF     | 0.29     | 5.0Â s     | +26Â % F1          |
| Precision    | **Crossâ€‘encoder rerank** (MiniLM)      | 0.31     | 5.4Â s     | +2Â pp F1          |
| Context      | **Topâ€‘3Â sentence compression** + dedup | 0.31     | 5.2Â s     | âˆ’300Â tokens avg   |
| Recall       | **RAGâ€‘Fusionâ€¯Ã—â€¯6Â subâ€‘queries**         | 0.34     | 10.5Â s    |                   |
| Voting       | **Ensemble** (BasicÂ âˆªÂ Fusion)          | **0.38** | 11.0Â s    | +65Â % vs baseline |

*Metric: tokenâ€‘level F1 on LitQAâ€‘v2 (49Â public test Qs). ScriptÂ â†’Â `notebooks/evaluate_litqa.ipynb`.*

---

## 3Â Â System architecture

```mermaid
graph TD
  Q["User Question"] -->|Condense| CQ["Condensed Query"]
  CQ -->|Dense| DENSE
  CQ -->|BM25| BM25
  DENSE --> RRF["RRF Fusion"]
  BM25 --> RRF
  RRF --> RR["Cross-Encoder Rerank"]
  RR --> CC["Context Compressor"]
  CC --> G["Llama-3-8B / Groq"]
  G --> SV["Self-Verification"]
  SV -->|VALID| OUT["Answer + Citations"]
  SV -->|INVALID| ALT["Re-answer (topâ€‘2 ctx)"]
  DENSE --retrieve--> DENSE_DB["FAISS HNSW"]
```

**Highlights**

* **FAISSÂ HNSW** index built from 49 PDFs (â‰ˆÂ 40Â k passages) embedded by *MedEmbedâ€‘large*.
* **Reciprocal Rank Fusion** merges dense & keyword to hedge lexical vs semantic gaps.
* **Pluggable reranker** (MiniLM â†”Â `BAAI/bgeâ€‘rerankerâ€‘base`) requires oneÂ line to swap.
* **Guardrail**: second Llamaâ€‘3 call marks answer *VALID/INVALID*; fallâ€‘back on INVALID.

---

## 4Â Â Repo layout

```
.
â”œâ”€â”€ app.py                 # Streamlit frontâ€‘end
â”œâ”€â”€ embed_pdf.py           # PDF â†’ FAISS builder
â”œâ”€â”€ llm_helper_groq.py     # LangChain chains
â”œâ”€â”€ notebooks/             # Jupyter eval & ablations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/              # original documents (gitâ€‘ignored)
â”‚   â””â”€â”€ faiss/             # autogenerated index
â””â”€â”€ requirements.txt
```

---

## 5Â Â Technology stack

* **PythonÂ 3.11** â€¢ **LangChain** â€¢ **FAISS** â€¢ **Whoosh BM25**
* **Sentenceâ€‘Transformers** (MedEmbed) â€¢ **crossâ€‘encoder / BGE reranker**
* **Groq Llamaâ€‘3â€‘8B** for lowâ€‘latency inference
* **Streamlit** & **FastAPI** for UI and API layer

---

> Â©Â 2025Â YourÂ NameÂ â€” Built for learning & realâ€‘world biomedical QA.

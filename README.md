# ğŸ©ºÂ Medical RAG AssistantÂ â€” LitQAâ€‘v2 Edition

> **From baseline to productionâ€‘ready:**Â +65â€¯% F1, citations on every answer,Â â‰¤â€¯11â€¯s median latency.

---

## 1Â Â Why this project matters

Biomedical questions need **upâ€‘toâ€‘date, fully cited** answers. The original *PaperHelper* demo proved the concept but shipped with:

* Denseâ€‘only retrieval â‡’ weak recall
* No reranking â‡’ noisy contexts
* Hallucinations in answers

This repo rebuilds the stack and adds measurable upgrades that raise LitQAâ€‘v2 scores from **0.23Â â†’Â 0.38Â F1** *without* expensive model fineâ€‘tunes.

---

## 2Â Â Key achievements

| Phase        | Upgrade                                | F1Â â–²     | LatencyÂ â–¼ | Notes             |
| ------------ | -------------------------------------- | -------- | --------- | ----------------- |
| **Baseline** | *PaperHelper* clone (denseâ€‘10)         | 0.23     | 4.8Â s     | public reference  |
| Retrieval    | **Hybrid** denseÂ 8Â +Â BM25Â 32 â€¢ RRF     | 0.29     | 5.0Â s     | +26Â % F1          |
| Precision    | **Crossâ€‘encoder rerank** (MiniLM)      | 0.31     | 5.4Â s     | +2Â pp F1          |
| Context      | **Topâ€‘3Â sentence compression** + dedup | 0.31     | 5.2Â s     | âˆ’300Â tokens avg   |
| Recall       | **RAGâ€‘Fusionâ€¯Ã—â€¯6Â subâ€‘queries**         | 0.34     | 10.5Â s    |                   |
| Voting       | **Ensemble** (BasicÂ âˆªÂ Fusion)          | **0.38** | 11.0Â s    | +65Â % vs baseline |

*Metric: tokenâ€‘level F1 on LitQAâ€‘v2 (49Â public test Qs). ScriptÂ â†’Â `notebooks/evaluate_litqa.ipynb`.*

---

## 3Â Â System architecture

```mermaid
graph TD
    Q[User Question] -->|LLM condense| CQ[CondensedÂ Query]
    CQ -->|Dense(8)| DENSE
    CQ -->|BM25(32)| BM25
    DENSE --> RRF
    BM25 --> RRF
    RRF --> RR[Crossâ€‘EncoderÂ Rerank]
    RR --> CC[ContextÂ Compressor]\n(dedupe + topâ€‘3 sent)
    CC --> G[GroqÂ Llamaâ€‘3â€‘8B]\n(answerâ€‘only JSON)
    G --> SV[Selfâ€‘VerificationÂ Guard]
    SV -->|VALID| OUT[AnswerÂ +Â Citations]
    SV -->|INVALID| ALT[Reâ€‘answerÂ withÂ topâ€‘2 ctx]

    subgraph VectorÂ DB
        style VectorÂ DB fill:#f3f3f3,stroke:#999,stroke-width:1px
        DENSE_DB[(FAISSÂ HNSWÂ 40Â k passages)]
    end
    DENSE --retrieve--> DENSE_DB
```

**Highlights**

* **FAISSÂ HNSW** index built from 49 PDFs (â‰ˆÂ 40Â k passages) embedded by *MedEmbedâ€‘large*.
* **Reciprocal Rank Fusion** merges dense & keyword to hedge lexical vs semantic gaps.
* **Pluggable reranker** (MiniLM â†”Â `BAAI/bgeâ€‘rerankerâ€‘base`) requires oneÂ line to swap.
* **Guardrail**: second Llamaâ€‘3 call marks answer *VALID/INVALID*; fallâ€‘back on INVALID.

---

## 4Â Â Repo layout

```
.
â”œâ”€â”€ app.py                 # Streamlit frontâ€‘end
â”œâ”€â”€ embed_pdf.py           # PDF â†’ FAISS builder
â”œâ”€â”€ llm_helper_groq.py     # LangChain chains
â”œâ”€â”€ notebooks/             # Jupyter eval & ablations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/              # original documents (gitâ€‘ignored)
â”‚   â””â”€â”€ faiss/             # autogenerated index
â””â”€â”€ requirements.txt
```

---

## 5Â Â Technology stack

* **PythonÂ 3.11** â€¢ **LangChain** â€¢ **FAISS** â€¢ **Whoosh BM25**
* **Sentenceâ€‘Transformers** (MedEmbed) â€¢ **crossâ€‘encoder / BGE reranker**
* **Groq Llamaâ€‘3â€‘8B** for lowâ€‘latency inference
* **Streamlit** & **FastAPI** for UI and API layer

---

> Â©Â 2025Â YourÂ NameÂ â€” Built for learning & realâ€‘world biomedical QA.

# ðŸ©ºÂ Medical RAG AssistantÂ â€” LitQAâ€‘v2 Edition

> **From baseline to productionâ€‘ready:**Â +65â€¯% F1, citations on every answer,Â â‰¤â€¯11â€¯s median latency.

---

## 1Â Â Why this project matters

Biomedical questions need **upâ€‘toâ€‘date, fully cited** answers. The original *PaperHelper* demo proved the concept but shipped with:

* Denseâ€‘only retrieval â‡’ weak recall
* No reranking â‡’ noisy contexts
* Hallucinations in answers

This repo rebuilds the stack and adds measurable upgrades that raise LitQAâ€‘v2 scores from **0.23Â â†’Â 0.38Â F1** *without* expensive model fineâ€‘tunes.

---

## 2Â Â Key achievements

| Phase        | Upgrade                                | F1Â â–²     | LatencyÂ â–¼ | Notes             |
| ------------ | -------------------------------------- | -------- | --------- | ----------------- |
| **Baseline** | *PaperHelper* clone (denseâ€‘10)         | 0.23     | 4.8Â s     | public reference  |
| Retrieval    | **Hybrid** denseÂ 8Â +Â BM25Â 32 â€¢ RRF     | 0.29     | 5.0Â s     | +26Â % F1          |
| Precision    | **Crossâ€‘encoder rerank** (MiniLM)      | 0.31     | 5.4Â s     | +2Â pp F1          |
| Context      | **Topâ€‘3Â sentence compression** + dedup | 0.31     | 5.2Â s     | âˆ’300Â tokens avg   |
| Recall       | **RAGâ€‘Fusionâ€¯Ã—â€¯6Â subâ€‘queries**         | 0.34     | 10.5Â s    |                   |
| Voting       | **Ensemble** (BasicÂ âˆªÂ Fusion)          | **0.38** | 11.0Â s    | +65Â % vs baseline |

*Metric: tokenâ€‘level F1 on LitQAâ€‘v2 (49Â public test Qs). ScriptÂ â†’Â `notebooks/evaluate_litqa.ipynb`.*

---

## 3Â Â System architecture

```mermaid
graph TD
  Q[User&nbsp;Question] -->|LLM&nbsp;condense| CQ[Condensed&nbsp;Query]
  CQ -->|Dense&nbsp;(8)| DENSE
  CQ -->|BM25&nbsp;(32)| BM25
  DENSE --> RRF[RRF&nbsp;Fusion]
  BM25 --> RRF
  RRF --> RR[Crossâ€‘Encoder&nbsp;Rerank]
  RR --> CC[Context&nbsp;Compressor]
  CC --> G[Groq&nbsp;Llamaâ€‘3â€‘8B]
  G --> SV[Selfâ€‘Verification]
  SV -->|VALID| OUT[Answer&nbsp;+&nbsp;Citations]
  SV -->|INVALID| ALT[Reâ€‘answer<br/>(topâ€‘2&nbsp;ctx)]
  DENSE --retrieve--> DENSE_DB[(FAISS&nbsp;HNSW)]
```

**Highlights**

* **FAISSÂ HNSW** index built from 49 PDFs (â‰ˆÂ 40Â k passages) embedded by *MedEmbedâ€‘large*.
* **Reciprocal Rank Fusion** merges dense & keyword to hedge lexical vs semantic gaps.
* **Pluggable reranker** (MiniLM â†”Â `BAAI/bgeâ€‘rerankerâ€‘base`) requires oneÂ line to swap.
* **Guardrail**: second Llamaâ€‘3 call marks answer *VALID/INVALID*; fallâ€‘back on INVALID.

---

## 4Â Â Repo layout

```
.
â”œâ”€â”€ app.py                 # Streamlit frontâ€‘end
â”œâ”€â”€ embed_pdf.py           # PDF â†’ FAISS builder
â”œâ”€â”€ llm_helper_groq.py     # LangChain chains
â”œâ”€â”€ notebooks/             # Jupyter eval & ablations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/              # original documents (gitâ€‘ignored)
â”‚   â””â”€â”€ faiss/             # autogenerated index
â””â”€â”€ requirements.txt
```

---

## 5Â Â Technology stack

* **PythonÂ 3.11** â€¢ **LangChain** â€¢ **FAISS** â€¢ **Whoosh BM25**
* **Sentenceâ€‘Transformers** (MedEmbed) â€¢ **crossâ€‘encoder / BGE reranker**
* **Groq Llamaâ€‘3â€‘8B** for lowâ€‘latency inference
* **Streamlit** & **FastAPI** for UI and API layer

---

> Â©Â 2025Â YourÂ NameÂ â€” Built for learning & realâ€‘world biomedical QA.
